In this section, we'll talk more about what parameter-efficient fine-tuning is.
Parameter-efficient fine-tuning is often abbreviated as PEFT. Parameter efficiency
encompasses many aspects including storage, memory, computation and also performance.
There are three categories of PEFT, including additive, selective and re-parameterization.
In this course, we'll focus only on the first and the third category,
which is additive and also re-parameterization because the selective category is found to be
not as good as the other two. But if you are interested in the selective category methods,
feel free to check out the links that I've have provided in the slide deck.
PEFT has been a very actively researched area.
At its core, additive methods include adding new tunable layers to the model. And during fine-tuning
process, we only update those new layer weights, while keeping the foundation model weights frozen.
Re-parameterization involves decomposing a weight matrix into lower-rank matrices.
We dive more deeply into both of these methods in the slides to come.
Fundamentally, these methods act on the core Transformer block; some act specifically on
the query, key, value weight matrices that are responsible for passing information.
Without further ado, let's talk about soft prompt. If you recall from the last section, the manual
act of writing text prompts is also called hard prompts or discrete prompts. In this section, it
is all about how to remove that manual aspect of prompt engineering but use soft prompts instead.
Adding soft prompts means that we're adding virtual tokens to our inputs. Take a look at
a graphic over here, where I have text inputs. So we often call these as text embeddings
"these chips are tasty" but soft prompt means that we are now adding virtual tokens.
And we'll talk a little bit more about virtual tokens in the next slide as well. But for now,
just know that these virtual tokens are task-specific So whenever I mention soft prompt,
you should immediately think of soft prompt as virtual tokens and they are synonymous.
So the soft prompt has the same dimension as our input embedding vectors. So you can see
that it spans the same length over here from top to bottom. We concatenate this trainable
virtual prompt or the virtual tokens or the soft prompt with the input embedding
vectors during fine-tuning process. And we call this prompt tuning, rather than model tuning,
because we are only updating the prompt weights, which is the ones in pink over here.
So what are those virtual tokens? Remember that the challenging part with prompt
engineering or writing discrete prompts was that it's very manual and very labor
intensive and very error-prone. So instead of relying on humans crafting a perfect prompt,
in soft prompt or virtual tokens, we let the model find the best prompt through fine-tuning.
We first randomly initialize an embedding vector that's just made up of random numbers
and this vector will have the same dimension as our input embeddings.
So these randomly initialized embedding vectors, since they are completely random,
so they are not part of any vocabulary. We don't know what text they correspond to.
So on the right-hand side, the graphic over here, you can see that when we have real word input
tokens, we can visualize them in an embedding space and know exactly what word it represents,
what token it represents. But when we look at virtual tokens in an embedding space,
we know that it occupies somewhere in the embedding space but we don't know what text
it corresponds to. So it's a little bit like Bitcoin when we know that it functions like
money but we cannot touch it like cash; we don't even know how it looks but it exists and it works.
In some research,
we have also seen people initialize these virtual tokens to represent discrete prompts. It means
that we are providing some minimal discrete prompts for the model to start with and then
the model is free to update embedding vectors during the training process. So for example, my
discrete prompt can be as simple as a three-word prompt "classify this sentence" or "translate this
sentence" and those discrete prompts will then be free to be updated as the model is fine-tuned.
So that initialization to discrete prompt is also called as informed initialization.
But what is interesting is that this paper has also found that random initialization, which
is when we set the prompts to random numbers, instead of specific text input, is nearly as
good as giving an informed initialization, which is when we pass in really simple prompt input,
text prompt input like "classify this text" or "translate this text".
So in the notebook later on, we'll play with both types of initialization: random
initialization and this informed or discrete prompt initialization. So that will help reinforce
or make the ideas of random initialization and informed initialization more concrete.
Let's now first take a look at what prompt tuning involves by comparing it with the
full fine-tuning scenario. We're sticking to the sentiment classification scenario.
Now, instead of having just one input, we are having multiple inputs, which is why
we're defining the task batch to be 4. Because we have four sentiments to classify in total,
notice that virtual tokens are really just random numbers, so they don't correspond to any specific
vocabulary or specific text. So just like fine-tuning, when we do full fine-tuning,
we update model weights based on loss through backprop but the entire foundation model is
unlocked or unfrozen so that we can update all the model weights during the backprop process.
In contrast to prompt tuning, you can see in this diagram that the foundation model weights
are completely frozen. So when the model goes through forward pass and also backprop, we only
update the virtual token weights here. So these virtual tokens or these soft prompts are basically
learned through backprop and tuned to incorporate signals from any number of labeled prompts that we
provide but. And we only apply the gradient updates to these virtual embedding vectors.
To recap, when we think about manual prompt engineering or discrete prompt writing,
which is when we supply just like in a few-shot learning example, where we supply some examples
and pass in as context to the LLM. That's when we are searching the text space over
tokens with fixed embeddings but with prompt tuning, we are searching in the embedding
space to find the best representation of the prompt that the LLMs should accept.
And the best thing out of this is the model learns the optimal representation
of the prompt automatically. So that removes the manual labor of engineering a discrete prompt.
So far, our example only includes a single example of task:
sentiment classification. But what if we have multiple tasks including Q/A, translation,
etc.?That's not a problem because we can treat each task as a prompt. For each prompt,
we would specify that for each task. So for each task, there will be a different soft
prompt altogether, so that way at deployment time, we don't need to swap in and out the base model or
the foundation model. What we need to do is to only swap in and out the learned virtual tokens.
The other benefit is that the model can now process a single larger mixed task batch.
And a task batch over here can consist of multiple requests. You can see now it not only
has sentiment classification but it can also take in a Q/A requests or translation requests. And
the variety of tasks that is captured in a batch is what we call as a mixed task batch over here.
What researchers found is that for larger models, especially at more than 11 billion
scale parameter, prompt tuning matches the performance of full fine-tuning.
SuperGLUE score is a benchmark that's styled after GLUE that contains a variety of tasks, including
answering boolean or comprehension questions. So the higher the score, the better it is.
We can see that soft prompt tuning doesn't perform as well when a model is small,
which intuitively makes sense because smaller models have smaller capacities to learn.
But when the model gets larger, then we can see that, with full fine-tuning versus prompt tuning,
we can see the performance actually is very similar. And I also want to call out with this
image that, prompt design is referring to the manual prompt engineering that we have to do.
Prompt length also doesn't really make that much of a difference for large model performance.
So what is prompt length? We can see in this example, when we initialize our virtual prompts,
there are only two embedding vectors over here. So this is a prompt length of two.
Larger models do well even when the prompt length is one. And in fact, there seems to be
diminishing return, diminishing return in model performance as we increase the prompt length.
A prompt length of 100, which is the green one over here, seems to be the sweet spot
but also notice that the confidence bar for the scores for the SuperGLUE score is
also quite wide. So, soft prompt tuning has the problem of unstable performance.
Let's recap the advantages of prompt tuning. Unlike few-shot learning, where we have to do
manual prompt engineering, we are not limited by the number of examples that we can pass to
the model context. We also eliminate the challenge of crafting the best prompt manually. We can use
backprop to ask the model to help us find the best embedding representation of the
task-specific virtual prompts. We don't need to have multiple copies of the same model.
We can afford to have multitask serving and lastly, it is also resilient to domain shift.
What this refers to is, since we freeze or we lock the foundation model weights,
prompt tuning prevents the model from modifying its general understanding of language. Therefore,
it reduces the model's ability to overfit on your fine-tuned data set.
By comparison, our learned soft prompts also have a much smaller number of parameters,
so they can be more generalizable to the variations of the tasks at inference time.
Of course, as we saw earlier, prompt tuning has some disadvantages. You'll probably have been
wondering the whole time or even in the beginning too, you know, how do you know what these virtual
tokens are? The answer is we don't really. The best attempt interpreting them is to use some
cosine distance or some distance metric to find out nearest neighbors so we can estimate or guess
what words they might represent. So it is much less interpretable compared to discrete prompt.
The second disadvantage, as we saw just now, is prompt tuning can have unstable performance.
Under the category of soft prompt, there's another very similar method called prefix
tuning. So prefix tuning is very similar to prompt tuning. It also allows task-specific prompts,
where each prefix represents a different task. The only difference is that these prefix
layers are added to each Transformer block, rather than just the input embedding layer.
So this wraps up our discussion on the additive category of PEFT, which is soft prompt.
In the upcoming section, we'll talk about re-parameterization, specifically LoRA.